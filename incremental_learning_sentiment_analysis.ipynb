{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHDqhbBApZDB"
      },
      "source": [
        "**!Download and Load**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ofj-UZWWlwXr",
        "outputId": "b43eb3de-11ed-4bd9-df36-09537983ad22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.6.14)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.16.0)\n",
            "Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2024.6.2)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle) (4.66.4)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.0.7)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle) (6.1.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.7)\n",
            "Dataset URL: https://www.kaggle.com/datasets/bittlingmayer/amazonreviews\n",
            "License(s): unknown\n",
            "Downloading amazonreviews.zip to /content\n",
            " 99% 490M/493M [00:24<00:00, 26.3MB/s]\n",
            "100% 493M/493M [00:24<00:00, 20.8MB/s]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip install kaggle\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle datasets download -d bittlingmayer/amazonreviews\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzkOXYyjpBu7",
        "outputId": "be0c5293-52e9-4a29-b606-6119ec488101"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  amazonreviews.zip\n",
            "  inflating: test.ft.txt.bz2         \n",
            "  inflating: train.ft.txt.bz2        \n"
          ]
        }
      ],
      "source": [
        "!unzip amazonreviews.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RPgOld-Wn5oz"
      },
      "outputs": [],
      "source": [
        "!mkdir dataset\n",
        "!bunzip2 -c /content/test.ft.txt.bz2 > /content/dataset/test.ft.txt\n",
        "!bunzip2 -c /content/train.ft.txt.bz2 > /content/dataset/train.ft.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dsM0U9wRoD2k"
      },
      "outputs": [],
      "source": [
        "!rm test.ft.txt.bz2\n",
        "!rm train.ft.txt.bz2\n",
        "!rm amazonreviews.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I3U2AxfDDCz4"
      },
      "outputs": [],
      "source": [
        "import gensim.downloader as api"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-L1MmUINDsKL",
        "outputId": "72668559-eeff-48ba-bb46-e919023a7efe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "__testing_word2vec-matrix-synopsis (-1 records): [THIS IS ONLY FOR TESTING] Word vecrors ...\n",
            "conceptnet-numberbatch-17-06-300 (1917247 records): ConceptNet Numberbatch consists of state...\n",
            "fasttext-wiki-news-subwords-300 (999999 records): 1 million word vectors trained on Wikipe...\n",
            "glove-twitter-100 (1193514 records): Pre-trained vectors based on  2B tweets,...\n",
            "glove-twitter-200 (1193514 records): Pre-trained vectors based on 2B tweets, ...\n",
            "glove-twitter-25 (1193514 records): Pre-trained vectors based on 2B tweets, ...\n",
            "glove-twitter-50 (1193514 records): Pre-trained vectors based on 2B tweets, ...\n",
            "glove-wiki-gigaword-100 (400000 records): Pre-trained vectors based on Wikipedia 2...\n",
            "glove-wiki-gigaword-200 (400000 records): Pre-trained vectors based on Wikipedia 2...\n",
            "glove-wiki-gigaword-300 (400000 records): Pre-trained vectors based on Wikipedia 2...\n",
            "glove-wiki-gigaword-50 (400000 records): Pre-trained vectors based on Wikipedia 2...\n",
            "word2vec-google-news-300 (3000000 records): Pre-trained vectors trained on a part of...\n",
            "word2vec-ruscorpora-300 (184973 records): Word2vec Continuous Skipgram vectors tra...\n"
          ]
        }
      ],
      "source": [
        "#check the list of models available\n",
        "info = api.info()\n",
        "for model_name, model_info in sorted(info['models'].items()):\n",
        "    print(\n",
        "        \"%s (%d records): %s\" % (\n",
        "            model_name,\n",
        "            model_info.get('num_records', -1),\n",
        "            model_info['description'][:40] +\"...\",\n",
        "        )\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzPK4Y2aFxj5",
        "outputId": "8c3e3069-d0ce-4a83-95ea-4dfc58da24fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 199.5/199.5MB downloaded\n"
          ]
        }
      ],
      "source": [
        "glove_model = api.load('glove-twitter-50')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TlB_C1Szq8n5",
        "outputId": "50c23536-6bac-4a17-a880-2516d9c092c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "import re\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L3YI6ntM7fMF"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from nltk.stem import WordNetLemmatizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRK6uolJqvep"
      },
      "source": [
        "**!Load and preoprocess data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v9D991Dmp2Ka"
      },
      "outputs": [],
      "source": [
        "train_dir = '/content/dataset/train.ft.txt'\n",
        "test_dir = '/content/dataset/test.ft.txt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q8112I-NoIZI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L_dbC194qudJ"
      },
      "outputs": [],
      "source": [
        "class TextDataGenerator:\n",
        "    def __init__(self, data_dir):\n",
        "        self.path = data_dir\n",
        "        self.pattern = r'^(__label__\\d+)\\s+(.*)$'\n",
        "        self.file = open(self.path, mode='r', encoding='utf-8')  # Open the file in read mode\n",
        "\n",
        "    def __iter__(self):\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        line = self.file.readline()\n",
        "        if not line:\n",
        "            self.file.seek(0) # upon exhausting the file, start from the begining again\n",
        "            raise StopIteration\n",
        "\n",
        "        match = re.match(self.pattern, line)\n",
        "        if match:\n",
        "            label = match.group(1).split('_')[-1]  # Extract label, removing \"__label__\"\n",
        "            label = 1 if label == '2' else 0 # convert label to 0 or 1\n",
        "            text = match.group(2).strip()  # Extract text, remove leading/trailing whitespace\n",
        "\n",
        "        return (label, text)\n",
        "\n",
        "    def __del__(self):\n",
        "        if hasattr(self, 'file') and self.file:\n",
        "            self.file.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PVJ7xXGgrUhs"
      },
      "outputs": [],
      "source": [
        "class Dataset:\n",
        "\n",
        "    def __init__(self, dir, dirname, batch = 64, wv = glove_model, ops =  'sum'):\n",
        "        self.path = dir\n",
        "        self.name = dirname\n",
        "        self.text_gen = TextDataGenerator(self.path)\n",
        "        self.nltk_stopwords = set(stopwords.words('english'))\n",
        "        self.nltk_punctuation = set(string.punctuation)\n",
        "        self.batch_size = batch\n",
        "        self.df = pd.DataFrame(columns=['text', 'label'])\n",
        "        self.lemmatizer = WordNetLemmatizer()\n",
        "        self.word_vectors = wv\n",
        "        self.op = ops\n",
        "\n",
        "\n",
        "    def __iter__(self):\n",
        "        return self\n",
        "\n",
        "\n",
        "    def __get_embeddings__(self, words ) :\n",
        "\n",
        "        \"\"\"Get embeddings of each word in a list\"\"\"\n",
        "        if self.op == 'sum' :\n",
        "            embedding_vector = []\n",
        "            for word in words :\n",
        "                if word in self.word_vectors:\n",
        "                    embedding_vector.append( self.word_vectors[word] )\n",
        "                else :\n",
        "                    embedding_vector.append( np.zeros(self.word_vectors.vector_size) )\n",
        "            embedding_vector = np.sum( np.array(embedding_vector), axis = 0)\n",
        "\n",
        "        elif self.op == 'mean' :\n",
        "            embedding_vector = []\n",
        "            for word in words :\n",
        "                if word in self.word_vectors:\n",
        "                    embedding_vector.append( self.word_vectors[word] )\n",
        "                else :\n",
        "                    embedding_vector.append( np.zeros(self.word_vectors.vector_size) )\n",
        "            embedding_vector = np.sum( np.array(embedding_vector), axis = 0)\n",
        "            embedding_vector = embedding_vector / len(words)\n",
        "\n",
        "        return embedding_vector\n",
        "\n",
        "    def __preprocess(self, sentence) :\n",
        "\n",
        "        \"\"\"Word tokenizes, removes stopwords and punctuation , apply lemmetization\"\"\"\n",
        "\n",
        "        #remove stop words and punctuation\n",
        "        words = nltk.word_tokenize(sentence)\n",
        "        filtered_words = [word for word in words if word not in self.nltk_stopwords and word not in self.nltk_punctuation]\n",
        "\n",
        "        ##apply lemmetization\n",
        "        lemmatized_words = [self.lemmatizer.lemmatize(word) for word in filtered_words]\n",
        "        return lemmatized_words\n",
        "\n",
        "    def __next__(self):\n",
        "\n",
        "        processed_data = []\n",
        "        labels = []\n",
        "        for i in range(self.batch_size) :\n",
        "\n",
        "            #Get a single item from the generator\n",
        "            label, text = next(self.text_gen)\n",
        "\n",
        "            # Tokenize text into sentences\n",
        "            sentences = nltk.sent_tokenize(text.lower())\n",
        "\n",
        "            # Tokenize each sentence into words, filter out stopwords and punctuation\n",
        "            filtered_sentences = []\n",
        "            for sentence in sentences:\n",
        "                filtered_words = self.__preprocess(sentence)\n",
        "                filtered_sentences.extend(filtered_words)\n",
        "                embeddings = self.__get_embeddings__(filtered_sentences)\n",
        "\n",
        "            # Append the preprocessed data to the list\n",
        "            processed_data.append(embeddings)\n",
        "            labels.append(label)\n",
        "\n",
        "        return (processed_data, labels)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jORRE816KqyH"
      },
      "source": [
        "**SVM**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gbm0ogKN_aZW"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tY9p12xpZgjP"
      },
      "outputs": [],
      "source": [
        "svm_data_train = Dataset(train_dir, 'train', batch = 5000)\n",
        "svm_data_test = Dataset(test_dir, 'test', batch = 1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I_QttxOKYlZC"
      },
      "outputs": [],
      "source": [
        "X, y = svm_data_train.__next__()\n",
        "X_t, y_t = svm_data_test.__next__()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RDpLMx7XYmwa",
        "outputId": "2f7021e4-491a-4e3e-d93a-9756f16b4a23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.731\n"
          ]
        }
      ],
      "source": [
        "clf = SVC(kernel = 'rbf', C=100, gamma=0.001)\n",
        "clf.fit(X, y)\n",
        "\n",
        "print( clf.score(X_t, y_t) )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nzZQcvxQEc1u"
      },
      "outputs": [],
      "source": [
        "params_grid = {\n",
        "    'C': [0.1, 1, 10, 100, 1000],\n",
        "    'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n",
        "    'kernel': ['rbf','linear','poly']\n",
        "}\n",
        "\n",
        "random_search = RandomizedSearchCV(SVC(), params_grid, n_iter=3, cv=3,verbose = 2)\n",
        "random_search = random_search.fit(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Op5P0NXd_J3M"
      },
      "outputs": [],
      "source": [
        "random_search.best_score_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8gcUHFEa_Or3"
      },
      "outputs": [],
      "source": [
        "best_clf = random_search.best_estimator_\n",
        "best_clf.score(X_t, y_t)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpkXsUCObvQF"
      },
      "source": [
        "Using Incremental learning to fit the entire dataset consisting of 3 million + examples\n",
        "#each batch will consist of 5000 sample , will fit the SGD for 100 batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rdjvse3J_UqQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9wPG7S6L67tY"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import SGDClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eu9LmZJS7U5w"
      },
      "outputs": [],
      "source": [
        "data_stream = Dataset(train_dir, 'train',batch=5000)\n",
        "test_data = Dataset(test_dir, 'test', batch = 1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AMcelWNbDEZZ"
      },
      "outputs": [],
      "source": [
        "X, y = data_stream.__next__()\n",
        "X_batch, y_batch = test_data.__next__()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vXGqLs5r69s_",
        "outputId": "60a5869c-0e01-4134-c183-e42d174372b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Base model 0.702\n"
          ]
        }
      ],
      "source": [
        "model = SGDClassifier(loss='hinge', penalty='l1', alpha=1e-7, random_state=42, max_iter=1000, tol=None)\n",
        "model.partial_fit(X, y, classes=np.unique(y) )\n",
        "\n",
        "print(f\"Base model {model.score(X_batch, y_batch)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mOV-xpTbCxE1",
        "outputId": "853d5de5-5d96-4b04-f33e-16438d07feea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " batch num : 1 Accuracy : 0.5780\n",
            " batch num : 11 Accuracy : 0.7150\n",
            " batch num : 21 Accuracy : 0.7350\n",
            " batch num : 31 Accuracy : 0.7180\n",
            " batch num : 41 Accuracy : 0.7040\n",
            " batch num : 51 Accuracy : 0.6530\n",
            " batch num : 61 Accuracy : 0.7050\n",
            " batch num : 71 Accuracy : 0.5790\n",
            " batch num : 81 Accuracy : 0.7420\n",
            " batch num : 91 Accuracy : 0.7240\n"
          ]
        }
      ],
      "source": [
        "accuracies = []\n",
        "\n",
        "for idx in range(100) :\n",
        "\n",
        "    # Predict with current model\n",
        "    # y_pred = model.predict(X)\n",
        "\n",
        "    # # Compute accuracy before updating\n",
        "    # accuracy_before = accuracy_score(y, y_pred)\n",
        "    # accuracies.append(accuracy_before)\n",
        "\n",
        "    X, y = data_stream.__next__()\n",
        "\n",
        "    batch_num = idx\n",
        "\n",
        "    # Update the model with the new data batch\n",
        "    model.partial_fit(X, y, classes=np.unique(y))\n",
        "\n",
        "    # Predict again after updating (optional, depending on your needs)\n",
        "    y_pred_after = model.predict(X_batch)\n",
        "\n",
        "    # Compute accuracy after updating (optional, depending on your needs)\n",
        "    accuracy_after = accuracy_score(y_batch, y_pred_after)\n",
        "    accuracies.append(accuracy_after)\n",
        "\n",
        "    if not batch_num % 10 :\n",
        "        print(f\" batch num : {batch_num + 1 } Accuracy : {accuracy_after:.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}