{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ahussain-ai/Sentiment-Analysis-Using-Incremental-Learning/blob/master/tf_sentiment_analysis_cnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U6UhA8VyD3qK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7PNGghw89uhQ"
      },
      "outputs": [],
      "source": [
        "import subprocess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1trDvJ4_855D"
      },
      "outputs": [],
      "source": [
        "# Commands to run\n",
        "commands = [\n",
        "    'pip install kaggle',\n",
        "    'mkdir -p ~/.kaggle',\n",
        "    'cp kaggle.json ~/.kaggle/',\n",
        "    'chmod 600 ~/.kaggle/kaggle.json',\n",
        "    'kaggle datasets download -d bittlingmayer/amazonreviews',\n",
        "    'unzip amazonreviews.zip',\n",
        "    'mkdir dataset',\n",
        "    'bunzip2 -c /content/test.ft.txt.bz2 > /content/dataset/test.ft.txt',\n",
        "    'bunzip2 -c /content/train.ft.txt.bz2 > /content/dataset/train.ft.txt',\n",
        "    'rm test.ft.txt.bz2',\n",
        "    'rm train.ft.txt.bz2',\n",
        "    'rm amazonreviews.zip'\n",
        "]\n",
        "\n",
        "# Execute each command\n",
        "for cmd in commands:\n",
        "    subprocess.run(cmd, shell=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ddLBCc3F9wd7"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "-OVwO1CF_XAX"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ZYNsLhgA99OT"
      },
      "outputs": [],
      "source": [
        "dataset_dir = '/content/dataset/train.ft.txt'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "be_jUcVjGovl"
      },
      "source": [
        "**1. Preprocess Text**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "faa49RXX-FME"
      },
      "outputs": [],
      "source": [
        "def preprocess_line(line):\n",
        "\n",
        "    pattern = r'^(__label__\\d+)\\s+(.*)$'\n",
        "    match = tf.strings.regex_full_match(line, pattern)\n",
        "\n",
        "    # Extract groups using tf.strings.regex_replace and capture groups\n",
        "    label = tf.strings.regex_replace(line, pattern, \"\\\\1\")\n",
        "    text = tf.strings.regex_replace(line, pattern, \"\\\\2\")\n",
        "\n",
        "    # Remove leading and trailing spaces from text\n",
        "    text = tf.strings.strip(text)\n",
        "\n",
        "    # Extract label from '__label__1' format\n",
        "    label = tf.strings.split(label, '__label__')[1]  # Split and get the second part\n",
        "    label = tf.strings.to_number(label,out_type=tf.int32)\n",
        "    label = tf.cond(tf.equal(label, 2), lambda: 1, lambda: 0)\n",
        "\n",
        "\n",
        "    return text, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "aAtJkp-H5jvW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "SsnIxLpP_Tf9"
      },
      "outputs": [],
      "source": [
        "\n",
        "#load train data\n",
        "dataset = tf.data.TextLineDataset(dataset_dir).map(preprocess_line, num_parallel_calls=tf.data.AUTOTUNE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "vqs3_3Jz5UKI"
      },
      "outputs": [],
      "source": [
        "#load test data\n",
        "test_dataset = tf.data.TextLineDataset('/content/dataset/test.ft.txt').map(preprocess_line, num_parallel_calls=tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "OkwtkoJN5UMn"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "6jzMAvG15UQG"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "fOUr1ijqnftd"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "fzNKDVqI5S_y"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "bOoHZ5fALh-H"
      },
      "outputs": [],
      "source": [
        "dataset = dataset.take(200000)\n",
        "test_dataset = test_dataset.take(100000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Oqo7UoQGs3B"
      },
      "source": [
        "**2.Vectorize and Tokanize**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "iiLqplkQEvhO"
      },
      "outputs": [],
      "source": [
        "tokenizer = tf.keras.layers.TextVectorization(max_tokens=20000, output_sequence_length = 128)\n",
        "tokenizer.adapt(dataset.map(lambda text, label: text))\n",
        "\n",
        "# Convert text to sequences within the Dataset pipeline\n",
        "max_length = 0\n",
        "def vectorize_text(text, label):\n",
        "    # nonlocal max_length\n",
        "    # text = tf.expand_dims(text, axis = 1)\n",
        "    tokenized_text = tokenizer(text)\n",
        "    # max_length = max(max_length, tf.shape(tokenized_text)[1])\n",
        "    return tokenizer(text), label\n",
        "\n",
        "vectorized_ds = dataset.map(vectorize_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "25OCC7NzgN1S"
      },
      "outputs": [],
      "source": [
        "#vcetrorize the test data\n",
        "test_vectorized_ds = test_dataset.map(vectorize_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "BZjYUKByvZjm"
      },
      "outputs": [],
      "source": [
        "# for text, label in vectorized_ds.take(7):\n",
        "#     print(f\"text : {text.numpy()}\")\n",
        "#     print(f\"label : {label.numpy()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ptIPZMWVD-Nb"
      },
      "outputs": [],
      "source": [
        "import gensim.downloader as api"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8iuvsYUmD7OS",
        "outputId": "939e1eaa-0495-4e11-f1b5-1fe5cdd88ee3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "__testing_word2vec-matrix-synopsis (-1 records): [THIS IS ONLY FOR TESTING] Word vecrors ...\n",
            "conceptnet-numberbatch-17-06-300 (1917247 records): ConceptNet Numberbatch consists of state...\n",
            "fasttext-wiki-news-subwords-300 (999999 records): 1 million word vectors trained on Wikipe...\n",
            "glove-twitter-100 (1193514 records): Pre-trained vectors based on  2B tweets,...\n",
            "glove-twitter-200 (1193514 records): Pre-trained vectors based on 2B tweets, ...\n",
            "glove-twitter-25 (1193514 records): Pre-trained vectors based on 2B tweets, ...\n",
            "glove-twitter-50 (1193514 records): Pre-trained vectors based on 2B tweets, ...\n",
            "glove-wiki-gigaword-100 (400000 records): Pre-trained vectors based on Wikipedia 2...\n",
            "glove-wiki-gigaword-200 (400000 records): Pre-trained vectors based on Wikipedia 2...\n",
            "glove-wiki-gigaword-300 (400000 records): Pre-trained vectors based on Wikipedia 2...\n",
            "glove-wiki-gigaword-50 (400000 records): Pre-trained vectors based on Wikipedia 2...\n",
            "word2vec-google-news-300 (3000000 records): Pre-trained vectors trained on a part of...\n",
            "word2vec-ruscorpora-300 (184973 records): Word2vec Continuous Skipgram vectors tra...\n"
          ]
        }
      ],
      "source": [
        "#check the list of models available\n",
        "info = api.info()\n",
        "for model_name, model_info in sorted(info['models'].items()):\n",
        "    print(\n",
        "        \"%s (%d records): %s\" % (\n",
        "            model_name,\n",
        "            model_info.get('num_records', -1),\n",
        "            model_info['description'][:40] +\"...\",\n",
        "        )\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "SwOYeyqkBTI8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AAn_HyCTBT-k"
      },
      "source": [
        "**Calculate embeddings of words in vocabulary**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uSp61rz7ECmm",
        "outputId": "e9e695bf-60f8-4cc0-fb34-ca31028d1589"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 199.5/199.5MB downloaded\n"
          ]
        }
      ],
      "source": [
        "glove_model = api.load('glove-twitter-50')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "QTHNJD3jDM8H"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "KEaqB3z9EHE5"
      },
      "outputs": [],
      "source": [
        "#find embeddings using glove50\n",
        "embedding_vector = np.zeros((len(tokenizer.get_vocabulary()), 50))\n",
        "dummy_embedding = np.zeros((50))\n",
        "for word in tokenizer.get_vocabulary():\n",
        "    if word in glove_model:\n",
        "        embedding_vector[tokenizer.get_vocabulary().index(word)] = glove_model[word]\n",
        "    else :\n",
        "        embedding_vector[tokenizer.get_vocabulary().index(word)] = dummy_embedding\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2gR0pGguEBeK",
        "outputId": "d0f509cc-168c-4b7a-e53c-03d5b9a37a57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 994 rows that are all zeros.\n"
          ]
        }
      ],
      "source": [
        "# Check which rows are all zeros\n",
        "zero_rows = np.all(embedding_vector == 0, axis=1)\n",
        "# Count how many rows are all zeros\n",
        "num_zero_rows = np.sum(zero_rows)\n",
        "\n",
        "print(f\"There are {num_zero_rows} rows that are all zeros.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hwKsxfX6EM40",
        "outputId": "d6677c9e-912d-4bf5-d754-bab40a4ed901"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Indices of rows that are all zeros:\n"
          ]
        }
      ],
      "source": [
        "# Check which rows are all zeros\n",
        "zero_rows_indices = np.where(np.all(embedding_vector == 0, axis=1))[0]\n",
        "\n",
        "# Print the indices of rows that are all zeros\n",
        "print(\"Indices of rows that are all zeros:\")\n",
        "# print(zero_rows_indices)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5moJzXnG6QU"
      },
      "source": [
        "**3. Create Tf Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "mioj8VV6GzxV"
      },
      "outputs": [],
      "source": [
        "# Shuffle, batch, and prefetch for performance\n",
        "BATCH_SIZE = 64\n",
        "SHUFFLE_BUFFER_SIZE = 20000\n",
        "\n",
        "train_ds = vectorized_ds.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE).repeat()\n",
        "test_ds = test_vectorized_ds.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KZ6VVdHHTv1"
      },
      "source": [
        "**4.Define Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "juwsj2C2lfPg",
        "outputId": "b23de05c-8ce4-4ed6-ecb2-f1badf4f6a56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of text batch: (64, 128)\n",
            "Shape of label batch: (64,)\n",
            "Shape of text batch: (64, 128)\n",
            "Shape of label batch: (64,)\n"
          ]
        }
      ],
      "source": [
        "batch = next(iter(train_ds))\n",
        "\n",
        "# Get shapes of elements in the batch\n",
        "text_batch, label_batch = batch\n",
        "print(\"Shape of text batch:\", text_batch.shape)\n",
        "print(\"Shape of label batch:\", label_batch.shape)\n",
        "\n",
        "\n",
        "test_batch = next(iter(test_ds))\n",
        "test_batch, label = test_batch\n",
        "print(\"Shape of text batch:\", test_batch.shape)\n",
        "print(\"Shape of label batch:\", label.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "BEx6CZXFHWJz"
      },
      "outputs": [],
      "source": [
        "def ann_model(tokenizer) :\n",
        "\n",
        "    # Example model definition\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Embedding(input_dim=len(tokenizer.get_vocabulary()), output_dim=50, mask_zero=True, weights = [embedding_vector]),\n",
        "        tf.keras.layers.GlobalAveragePooling1D(),\n",
        "        tf.keras.layers.Dense(128, activation='relu'),\n",
        "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate = 0.004),\n",
        "                loss='binary_crossentropy',\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "Kx0PHMzw7AK9"
      },
      "outputs": [],
      "source": [
        "def cnn_model(tokenizer) :\n",
        "\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Embedding(input_dim=len(tokenizer.get_vocabulary()), output_dim=50,mask_zero = True, weights = [embedding_vector]),\n",
        "        tf.keras.layers.Conv1D(16, 7, activation='relu', kernel_regularizer='l2'),\n",
        "        tf.keras.layers.GlobalMaxPooling1D(),\n",
        "        tf.keras.layers.Dropout(0.5),\n",
        "        tf.keras.layers.Dense(8),\n",
        "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "\n",
        "        ])\n",
        "\n",
        "\n",
        "     # Compile the model\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate = 0.001),\n",
        "                loss='binary_crossentropy',\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ahPK-DEIGtgZ"
      },
      "source": [
        "**5.Training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BNnaDj6cHkVT",
        "outputId": "bfe5017f-fe4d-4db1-ceeb-c7c70e08d83f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, None, 50)          1000000   \n",
            "                                                                 \n",
            " global_average_pooling1d (  (None, 50)                0         \n",
            " GlobalAveragePooling1D)                                         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 128)               6528      \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 129       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1006657 (3.84 MB)\n",
            "Trainable params: 1006657 (3.84 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/5\n",
            "3000/3000 [==============================] - 203s 64ms/step - loss: 0.2794 - accuracy: 0.8837 - val_loss: 0.2754 - val_accuracy: 0.8880\n",
            "Epoch 2/5\n",
            "3000/3000 [==============================] - 120s 40ms/step - loss: 0.2290 - accuracy: 0.9076 - val_loss: 0.2663 - val_accuracy: 0.8934\n",
            "Epoch 3/5\n",
            "3000/3000 [==============================] - 151s 50ms/step - loss: 0.2027 - accuracy: 0.9178 - val_loss: 0.2871 - val_accuracy: 0.8930\n",
            "Epoch 4/5\n",
            "3000/3000 [==============================] - 155s 52ms/step - loss: 0.1777 - accuracy: 0.9279 - val_loss: 0.3082 - val_accuracy: 0.8894\n",
            "Epoch 5/5\n",
            "3000/3000 [==============================] - 143s 48ms/step - loss: 0.1497 - accuracy: 0.9393 - val_loss: 0.3808 - val_accuracy: 0.8888\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Train the ann model\n",
        "model = ann_model(tokenizer)\n",
        "model.summary()\n",
        "history = model.fit(train_ds, epochs=5,steps_per_epoch = 3000, validation_data = test_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rI4RSIdiInOP",
        "outputId": "ca8af204-152c-481c-9164-1200a5265089"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_2 (Embedding)     (None, None, 50)          1000000   \n",
            "                                                                 \n",
            " conv1d_1 (Conv1D)           (None, None, 16)          5616      \n",
            "                                                                 \n",
            " global_max_pooling1d_1 (Gl  (None, 16)                0         \n",
            " obalMaxPooling1D)                                               \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 16)                0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 8)                 136       \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 1)                 9         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1005761 (3.84 MB)\n",
            "Trainable params: 1005761 (3.84 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "cnn = cnn_model(tokenizer)\n",
        "cnn.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pThfZfRqInZz",
        "outputId": "5a5d9bb4-2144-42ae-b6df-1bf9964d1d6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "3000/3000 [==============================] - 187s 57ms/step - loss: 0.4953 - accuracy: 0.7985 - val_loss: 0.3466 - val_accuracy: 0.8752\n",
            "Epoch 2/10\n",
            "3000/3000 [==============================] - 121s 40ms/step - loss: 0.3802 - accuracy: 0.8599 - val_loss: 0.3208 - val_accuracy: 0.8851\n",
            "Epoch 3/10\n",
            "3000/3000 [==============================] - 119s 40ms/step - loss: 0.3567 - accuracy: 0.8700 - val_loss: 0.3126 - val_accuracy: 0.8903\n",
            "Epoch 4/10\n",
            "3000/3000 [==============================] - 156s 52ms/step - loss: 0.3443 - accuracy: 0.8770 - val_loss: 0.3339 - val_accuracy: 0.8802\n",
            "Epoch 5/10\n",
            "3000/3000 [==============================] - 112s 37ms/step - loss: 0.3336 - accuracy: 0.8821 - val_loss: 0.3119 - val_accuracy: 0.8921\n",
            "Epoch 6/10\n",
            "3000/3000 [==============================] - 114s 38ms/step - loss: 0.3227 - accuracy: 0.8863 - val_loss: 0.3243 - val_accuracy: 0.8837\n",
            "Epoch 7/10\n",
            "3000/3000 [==============================] - 155s 52ms/step - loss: 0.3150 - accuracy: 0.8909 - val_loss: 0.3125 - val_accuracy: 0.8921\n",
            "Epoch 8/10\n",
            "3000/3000 [==============================] - 107s 36ms/step - loss: 0.3064 - accuracy: 0.8937 - val_loss: 0.3135 - val_accuracy: 0.8913\n",
            "Epoch 9/10\n",
            "3000/3000 [==============================] - 125s 42ms/step - loss: 0.3012 - accuracy: 0.8973 - val_loss: 0.3177 - val_accuracy: 0.8903\n",
            "Epoch 10/10\n",
            "3000/3000 [==============================] - 108s 36ms/step - loss: 0.2935 - accuracy: 0.9005 - val_loss: 0.3314 - val_accuracy: 0.8881\n"
          ]
        }
      ],
      "source": [
        "history = cnn.fit(train_ds, epochs=10,steps_per_epoch = 3000, validation_data = test_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "33-3IeN5_2wM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e468b1ef-7b5e-48de-f0da-42faf19e27e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1563/1563 [==============================] - 39s 25ms/step - loss: 0.3314 - accuracy: 0.8881\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.33138883113861084, 0.8881000280380249]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "cnn.evaluate(test_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "kAwiB4EiIqvu"
      },
      "outputs": [],
      "source": [
        "sample = \"\"\"It's very useful product for me.I use it on daily basis.Till now this has been my favourite product which I have bought online.Blades\n",
        "r nice n sharp..but after few months string causes problem otherwise it's very handy in kitchen cutting chores\"\"\"\n",
        "\n",
        "tokenized_text = tokenizer(sample)\n",
        "# print(tokenized_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "TaPOhHXNI3DG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d6cdc8e-8b05-488c-fe32-474bb231722c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 27ms/step\n",
            "[[0.96174]]\n"
          ]
        }
      ],
      "source": [
        "prediction = cnn.predict(tf.expand_dims(tokenized_text, axis=0))\n",
        "print(prediction)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmZPw0BlntdY"
      },
      "source": [
        "**RNN for classification**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "RvMXHowinwoY"
      },
      "outputs": [],
      "source": [
        "def rnn_model(tokenizer) :\n",
        "\n",
        "    model = tf.keras.models.Sequential([\n",
        "        tf.keras.layers.Embedding(input_dim=len(tokenizer.get_vocabulary()), output_dim=50, mask_zero=True, weights = [embedding_vector]),\n",
        "        tf.keras.layers.SimpleRNN(64),  # Simple RNN layer with 64 units\n",
        "        tf.keras.layers.Dense(16, activation='relu'),\n",
        "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "        ])\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate = 0.0009),loss='binary_crossentropy',metrics=['accuracy'])\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "kWyHtMbinwrO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66bbc1c0-3753-4ce8-e963-a9e877ef526e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_3 (Embedding)     (None, None, 50)          1000000   \n",
            "                                                                 \n",
            " simple_rnn (SimpleRNN)      (None, 64)                7360      \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 16)                1040      \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1008417 (3.85 MB)\n",
            "Trainable params: 1008417 (3.85 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "rnn_model = rnn_model(tokenizer)\n",
        "rnn_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GFMSrpshnwua",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df4c83a5-d90b-4f35-fa92-572b13a69f53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "3000/3000 [==============================] - 1196s 394ms/step - loss: 0.6012 - accuracy: 0.6722 - val_loss: 0.5914 - val_accuracy: 0.6954\n",
            "Epoch 2/20\n",
            "3000/3000 [==============================] - 1190s 397ms/step - loss: 0.5254 - accuracy: 0.7418 - val_loss: 0.4577 - val_accuracy: 0.8085\n",
            "Epoch 3/20\n",
            "3000/3000 [==============================] - 1149s 383ms/step - loss: 0.4841 - accuracy: 0.7658 - val_loss: 0.4636 - val_accuracy: 0.7736\n",
            "Epoch 4/20\n",
            "3000/3000 [==============================] - 1176s 392ms/step - loss: 0.4152 - accuracy: 0.8160 - val_loss: 0.3878 - val_accuracy: 0.8243\n",
            "Epoch 5/20\n",
            "3000/3000 [==============================] - 1135s 379ms/step - loss: 0.4370 - accuracy: 0.8045 - val_loss: 0.4893 - val_accuracy: 0.7852\n",
            "Epoch 6/20\n",
            "3000/3000 [==============================] - 1142s 381ms/step - loss: 0.4241 - accuracy: 0.8178 - val_loss: 0.5207 - val_accuracy: 0.7319\n",
            "Epoch 7/20\n",
            "3000/3000 [==============================] - ETA: 0s - loss: 0.4607 - accuracy: 0.7881"
          ]
        }
      ],
      "source": [
        "rnn_model.fit(train_ds, epochs=20,steps_per_epoch = 3000, validation_data = test_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PHIH0Rm1Infj"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fweoLDXZInjF"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('seaborn')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iGRPhC-AybmX"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def plot_history(history):\n",
        "\n",
        "    # Get the loss and accuracy values from the history dictionary\n",
        "    loss = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "    accuracy = history.history['accuracy']\n",
        "    val_accuracy = history.history['val_accuracy']\n",
        "\n",
        "    # Create a figure with two subplots\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
        "\n",
        "    # Plot the loss values on the first subplot\n",
        "    ax1.plot(loss, label='Training Loss')\n",
        "    ax1.plot(val_loss, label='Validation Loss')\n",
        "    ax1.set_title('Loss Plot')\n",
        "    ax1.set_xlabel('Epoch')\n",
        "    ax1.set_ylabel('Loss')\n",
        "    ax1.legend()\n",
        "\n",
        "    # Plot the accuracy values on the second subplot\n",
        "    ax2.plot(accuracy, label='Training Accuracy')\n",
        "    ax2.plot(val_accuracy, label='Validation Accuracy')\n",
        "    ax2.set_title('Accuracy Plot')\n",
        "    ax2.set_xlabel('Epoch')\n",
        "    ax2.set_ylabel('Accuracy')\n",
        "    ax2.legend()\n",
        "\n",
        "    # Show the plot\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_history(history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CLmQaAJ1DtgZ"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyMWJNAVnOFE8gh/qwUDca9F",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}